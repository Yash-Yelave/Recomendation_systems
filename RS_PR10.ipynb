{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-Yelave/Recomendation_systems/blob/main/RS_PR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from collections import defaultdict\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. MOCK DATA & METADATA\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Item Metadata ---\n",
        "item_metadata = {\n",
        "    'item1': {'category': 'electronics', 'price': 100}, 'item2': {'category': 'apparel', 'price': 80},\n",
        "    'item3': {'category': 'electronics', 'price': 120}, 'item4': {'category': 'books', 'price': 25},\n",
        "    'item5': {'category': 'books', 'price': 30}, 'item6': {'category': 'books', 'price': 20},\n",
        "    'item7': {'category': 'apparel', 'price': 75}, 'item8': {'category': 'books', 'price': 60},\n",
        "    'item9': {'category': 'electronics', 'price': 45}, 'item10': {'category': 'books', 'price': 15}\n",
        "}\n",
        "all_items = list(item_metadata.keys())\n",
        "\n",
        "# --- User Interaction History (TRAINING DATA) ---\n",
        "# This is what the model \"knows\" about the user's past behavior.\n",
        "user_interaction_history = {\n",
        "    'user1': ['item1', 'item6'], # User 1 likes electronics and books\n",
        "    'user2': ['item2', 'item7'], # User 2 likes apparel\n",
        "    'user3': ['item4', 'item5'], # User 3 likes books\n",
        "}\n",
        "\n",
        "# --- Ground Truth (TEST DATA) ---\n",
        "# These are the *unseen* items that the user actually likes.\n",
        "# A good model should recommend these.\n",
        "ground_truth_ratings = {\n",
        "    'user1': {'item3': 5, 'item9': 4},\n",
        "    'user2': {'item4': 4, 'item5': 5},\n",
        "    'user3': {'item1': 3, 'item2': 4, 'item6': 5, 'item10': 4}\n",
        "}\n",
        "ground_truth_top_n = {\n",
        "    'user1': ['item3', 'item9'],\n",
        "    'user2': ['item4', 'item5'],\n",
        "    'user3': ['item1', 'item6', 'item8', 'item10']\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PLACEHOLDER RECOMMENDATION SYSTEM ALGORITHMS\n",
        "# ==============================================================================\n",
        "\n",
        "def collaborative_filtering_model(user_id):\n",
        "    \"\"\"Simulates a collaborative filtering model that predicts ratings for unseen items.\"\"\"\n",
        "    # Note: A real CF model would be trained on a larger dataset.\n",
        "    # This mock output is designed to have some accuracy for evaluation.\n",
        "    predictions = {\n",
        "        'user1': {'item3': 4.8, 'item9': 3.5, 'item2': 2.5, 'item4': 3.1},\n",
        "        'user2': {'item4': 4.1, 'item5': 4.9, 'item1': 3.0},\n",
        "        'user3': {'item1': 3.3, 'item2': 4.5, 'item6': 4.1, 'item10': 3.9, 'item8': 3.2}\n",
        "    }\n",
        "    return predictions.get(user_id, {})\n",
        "\n",
        "def content_based_model(user_id):\n",
        "    \"\"\"(IMPROVED) Recommends items based on the user's interaction history.\"\"\"\n",
        "    history = user_interaction_history.get(user_id, [])\n",
        "    if not history: return []\n",
        "\n",
        "    liked_categories = {item_metadata[item]['category'] for item in history if item in item_metadata}\n",
        "\n",
        "    scores = defaultdict(float)\n",
        "    for item in all_items:\n",
        "        if item in item_metadata and item_metadata[item]['category'] in liked_categories:\n",
        "            scores[item] += 1\n",
        "\n",
        "    # Filter out items the user has already seen\n",
        "    recommendations = {item: score for item, score in scores.items() if item not in history}\n",
        "    return sorted(recommendations.keys(), key=lambda x: recommendations[x], reverse=True)\n",
        "\n",
        "def knowledge_based_model(user_requirements):\n",
        "    \"\"\"Filters based on explicit rules.\"\"\"\n",
        "    recs = []\n",
        "    for item, metadata in item_metadata.items():\n",
        "        if all([\n",
        "            metadata.get('category') == user_requirements.get('category', metadata.get('category')),\n",
        "            metadata.get('price', float('inf')) <= user_requirements.get('max_price', float('inf'))\n",
        "        ]):\n",
        "            recs.append(item)\n",
        "    return recs\n",
        "\n",
        "def hybrid_model(user_id):\n",
        "    \"\"\"Simulates a hybrid model combining CF scores and CBF ranking.\"\"\"\n",
        "    history = user_interaction_history.get(user_id, [])\n",
        "    cf_preds = collaborative_filtering_model(user_id)\n",
        "    cb_preds_list = content_based_model(user_id)\n",
        "    cb_scores = {item: len(cb_preds_list) - i for i, item in enumerate(cb_preds_list)}\n",
        "\n",
        "    hybrid_scores = defaultdict(float)\n",
        "    max_cf = max(cf_preds.values()) if cf_preds else 1\n",
        "    for item, score in cf_preds.items():\n",
        "        hybrid_scores[item] += 0.5 * (score / max_cf)\n",
        "\n",
        "    max_cb = max(cb_scores.values()) if cb_scores else 1\n",
        "    for item, score in cb_scores.items():\n",
        "        hybrid_scores[item] += 0.5 * (score / max_cb)\n",
        "\n",
        "    filtered = {item: score for item, score in hybrid_scores.items() if item not in history}\n",
        "    return sorted(filtered.keys(), key=lambda x: filtered[x], reverse=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EVALUATION FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate_rating_prediction(y_true_ratings, y_pred_ratings_all_users):\n",
        "    \"\"\"Calculates MAE and RMSE.\"\"\"\n",
        "    true, pred = [], []\n",
        "    for user, items in y_true_ratings.items():\n",
        "        for item, rating in items.items():\n",
        "            if user in y_pred_ratings_all_users and item in y_pred_ratings_all_users[user]:\n",
        "                true.append(rating)\n",
        "                pred.append(y_pred_ratings_all_users[user][item])\n",
        "    if not true: return {\"mae\": float('nan'), \"rmse\": float('nan')}\n",
        "    return {\"mae\": mean_absolute_error(true, pred), \"rmse\": np.sqrt(mean_squared_error(true, pred))}\n",
        "\n",
        "def calculate_ranking_metrics(y_true_top_n, y_pred_top_n, k):\n",
        "    \"\"\"Calculates Accuracy, Precision, Recall, F1-Score, and NDCG at k.\"\"\"\n",
        "    metrics = defaultdict(list)\n",
        "    for user, true_items in y_true_top_n.items():\n",
        "        if user in y_pred_top_n and y_pred_top_n[user]:\n",
        "            pred = y_pred_top_n[user][:k]\n",
        "            true_set = set(true_items)\n",
        "            hits = len(true_set.intersection(set(pred)))\n",
        "\n",
        "            precision = hits / len(pred)\n",
        "            recall = hits / len(true_set) if true_set else 0.0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "            metrics['accuracy'].append(precision)\n",
        "            metrics['precision'].append(precision)\n",
        "            metrics['recall'].append(recall)\n",
        "            metrics['f1_score'].append(f1)\n",
        "\n",
        "            dcg = sum(1.0 / np.log2(i + 2) for i, item in enumerate(pred) if item in true_set)\n",
        "            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(k, len(true_set))))\n",
        "            metrics['ndcg'].append((dcg / idcg) if idcg > 0 else 0.0)\n",
        "\n",
        "    return {f\"{name}@{k}\": np.mean(values) for name, values in metrics.items()}\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MAIN EXECUTION & EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    K = 5\n",
        "    print(f\"ðŸš€ Recommendation System Evaluation (k={K}) ðŸš€\\n\")\n",
        "\n",
        "    # --- 4.1 Collaborative Filtering ---\n",
        "    print(\"## 1. Collaborative Filtering Evaluation\")\n",
        "    cf_preds = {user: collaborative_filtering_model(user) for user in ground_truth_top_n}\n",
        "    rating_metrics = evaluate_rating_prediction(ground_truth_ratings, cf_preds)\n",
        "    print(f\"  --> MAE:  {rating_metrics['mae']:.4f}, RMSE: {rating_metrics['rmse']:.4f} (Lower is better)\")\n",
        "\n",
        "    cf_top_n = {u: sorted(p.keys(), key=p.get, reverse=True) for u, p in cf_preds.items()}\n",
        "    ranking_metrics = calculate_ranking_metrics(ground_truth_top_n, cf_top_n, K)\n",
        "    for m, v in ranking_metrics.items(): print(f\"  --> {m}: {v:.4f} (Higher is better)\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # --- 4.2 Content-Based ---\n",
        "    print(\"\\n## 2. Content-Based Model Evaluation\")\n",
        "    print(\"  --> MAE/RMSE: Not applicable (does not predict ratings)\")\n",
        "    cb_top_n = {user: content_based_model(user) for user in ground_truth_top_n}\n",
        "    ranking_metrics = calculate_ranking_metrics(ground_truth_top_n, cb_top_n, K)\n",
        "    for m, v in ranking_metrics.items(): print(f\"  --> {m}: {v:.4f} (Higher is better)\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # --- 4.3 Knowledge-Based ---\n",
        "    print(\"\\n## 3. Knowledge-Based Model Evaluation\")\n",
        "    user3_reqs = {'category': 'books', 'max_price': 50}\n",
        "    kb_top_n = {'user3': knowledge_based_model(user3_reqs)}\n",
        "    print(f\"  --> Evaluating for user3 with requirements: {user3_reqs}\")\n",
        "    print(\"  --> MAE/RMSE: Not applicable (does not predict ratings)\")\n",
        "    ranking_metrics = calculate_ranking_metrics(ground_truth_top_n, kb_top_n, K)\n",
        "    for m, v in ranking_metrics.items(): print(f\"  --> {m}: {v:.4f} (Higher is better)\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # --- 4.4 Hybrid Model ---\n",
        "    print(\"\\n## 4. Hybrid Model Evaluation\")\n",
        "    print(\"  --> MAE/RMSE: Not applicable (does not predict ratings)\")\n",
        "    hybrid_top_n = {user: hybrid_model(user) for user in ground_truth_top_n}\n",
        "    ranking_metrics = calculate_ranking_metrics(ground_truth_top_n, hybrid_top_n, K)\n",
        "    for m, v in ranking_metrics.items(): print(f\"  --> {m}: {v:.4f} (Higher is better)\")\n",
        "    print(\"\\nâœ… Evaluation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jny6sECe9_KW",
        "outputId": "7fcd7e85-9096-49b1-caf5-539f4179281a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Recommendation System Evaluation (k=5) ðŸš€\n",
            "\n",
            "## 1. Collaborative Filtering Evaluation\n",
            "  --> MAE:  0.3375, RMSE: 0.4287 (Lower is better)\n",
            "  --> accuracy@5: 0.6556 (Higher is better)\n",
            "  --> precision@5: 0.6556 (Higher is better)\n",
            "  --> recall@5: 1.0000 (Higher is better)\n",
            "  --> f1_score@5: 0.7852 (Higher is better)\n",
            "  --> ndcg@5: 0.9202 (Higher is better)\n",
            "---------------------------------------------\n",
            "\n",
            "## 2. Content-Based Model Evaluation\n",
            "  --> MAE/RMSE: Not applicable (does not predict ratings)\n",
            "  --> accuracy@5: 0.7000 (Higher is better)\n",
            "  --> precision@5: 0.7000 (Higher is better)\n",
            "  --> recall@5: 0.8750 (Higher is better)\n",
            "  --> f1_score@5: 0.7143 (Higher is better)\n",
            "  --> ndcg@5: 0.8411 (Higher is better)\n",
            "---------------------------------------------\n",
            "\n",
            "## 3. Knowledge-Based Model Evaluation\n",
            "  --> Evaluating for user3 with requirements: {'category': 'books', 'max_price': 50}\n",
            "  --> MAE/RMSE: Not applicable (does not predict ratings)\n",
            "  --> accuracy@5: 0.5000 (Higher is better)\n",
            "  --> precision@5: 0.5000 (Higher is better)\n",
            "  --> recall@5: 0.5000 (Higher is better)\n",
            "  --> f1_score@5: 0.5000 (Higher is better)\n",
            "  --> ndcg@5: 0.3633 (Higher is better)\n",
            "---------------------------------------------\n",
            "\n",
            "## 4. Hybrid Model Evaluation\n",
            "  --> MAE/RMSE: Not applicable (does not predict ratings)\n",
            "  --> accuracy@5: 0.6222 (Higher is better)\n",
            "  --> precision@5: 0.6222 (Higher is better)\n",
            "  --> recall@5: 1.0000 (Higher is better)\n",
            "  --> f1_score@5: 0.7534 (Higher is better)\n",
            "  --> ndcg@5: 0.9675 (Higher is better)\n",
            "\n",
            "âœ… Evaluation complete.\n"
          ]
        }
      ]
    }
  ]
}